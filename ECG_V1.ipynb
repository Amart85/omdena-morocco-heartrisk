{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c10cceb",
   "metadata": {},
   "source": [
    "## ECG Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03360466",
   "metadata": {},
   "source": [
    "### Intorduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01c1c9",
   "metadata": {},
   "source": [
    "The project, led by Omdena Morocco chapter aims to predict or interpret ECG, early, using an application. This could help decrease the number of deaths that occur due to delay of diagnosis or even mis-interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60897fb",
   "metadata": {},
   "source": [
    "* [Step 0](#step0): Import Dataset\n",
    "* [Step 1](#step1): Split the data set\n",
    "* [Step 2](#step2): Creat Data Loaders\n",
    "* [Step 3](#step3): Creat Train Function\n",
    "* [Step 4](#step4): Create Test Function\n",
    "* [Step 5](#step5): Creat VGG Model\n",
    "* [Step 6](#step6): Set Hyper Parameters\n",
    "* [Step 7](#step7): Train The model\n",
    "* [Step 8](#step8): Test The model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb1947",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "## Step 0: Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819402f",
   "metadata": {},
   "source": [
    "### Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3a18a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 928 total  images, with different classes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# load filenames for all images\n",
    "ECG_files = np.array(glob(\"./DS6/*/*\"))\n",
    "\n",
    "\n",
    "# print number of images in the dataset\n",
    "print('There are %d total  images, with different classes' % len(ECG_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77977a5",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Split the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27c667",
   "metadata": {},
   "source": [
    "Here I have splitted the available data into test(10%), train(80%) and validation(10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e411a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying files: 0 files [00:00, ? files/s]\u001b[A\n",
      "Copying files: 6 files [00:00, 54.67 files/s]\u001b[A\n",
      "Copying files: 18 files [00:00, 89.23 files/s]\u001b[A\n",
      "Copying files: 29 files [00:00, 97.62 files/s]\u001b[A\n",
      "Copying files: 40 files [00:00, 100.89 files/s]\u001b[A\n",
      "Copying files: 51 files [00:00, 103.98 files/s]\u001b[A\n",
      "Copying files: 62 files [00:00, 105.92 files/s]\u001b[A\n",
      "Copying files: 75 files [00:00, 111.28 files/s]\u001b[A\n",
      "Copying files: 87 files [00:00, 109.47 files/s]\u001b[A\n",
      "Copying files: 99 files [00:00, 112.32 files/s]\u001b[A\n",
      "Copying files: 111 files [00:01, 112.68 files/s]\u001b[A\n",
      "Copying files: 124 files [00:01, 116.00 files/s]\u001b[A\n",
      "Copying files: 136 files [00:01, 105.23 files/s]\u001b[A\n",
      "Copying files: 147 files [00:01, 88.76 files/s] \u001b[A\n",
      "Copying files: 157 files [00:01, 84.15 files/s]\u001b[A\n",
      "Copying files: 168 files [00:01, 88.98 files/s]\u001b[A\n",
      "Copying files: 178 files [00:01, 88.64 files/s]\u001b[A\n",
      "Copying files: 190 files [00:01, 92.77 files/s]\u001b[A\n",
      "Copying files: 200 files [00:02, 93.02 files/s]\u001b[A\n",
      "Copying files: 213 files [00:02, 102.05 files/s]\u001b[A\n",
      "Copying files: 224 files [00:02, 102.04 files/s]\u001b[A\n",
      "Copying files: 238 files [00:02, 110.68 files/s]\u001b[A\n",
      "Copying files: 250 files [00:02, 104.40 files/s]\u001b[A\n",
      "Copying files: 263 files [00:02, 110.22 files/s]\u001b[A\n",
      "Copying files: 275 files [00:02, 111.89 files/s]\u001b[A\n",
      "Copying files: 287 files [00:02, 111.14 files/s]\u001b[A\n",
      "Copying files: 299 files [00:02, 112.87 files/s]\u001b[A\n",
      "Copying files: 311 files [00:03, 114.66 files/s]\u001b[A\n",
      "Copying files: 323 files [00:03, 114.98 files/s]\u001b[A\n",
      "Copying files: 335 files [00:03, 114.85 files/s]\u001b[A\n",
      "Copying files: 348 files [00:03, 116.65 files/s]\u001b[A\n",
      "Copying files: 361 files [00:03, 118.87 files/s]\u001b[A\n",
      "Copying files: 373 files [00:03, 104.79 files/s]\u001b[A\n",
      "Copying files: 384 files [00:03, 87.81 files/s] \u001b[A\n",
      "Copying files: 394 files [00:03, 90.36 files/s]\u001b[A\n",
      "Copying files: 405 files [00:03, 93.51 files/s]\u001b[A\n",
      "Copying files: 415 files [00:04, 82.34 files/s]\u001b[A\n",
      "Copying files: 424 files [00:04, 77.81 files/s]\u001b[A\n",
      "Copying files: 433 files [00:04, 80.06 files/s]\u001b[A\n",
      "Copying files: 444 files [00:04, 86.70 files/s]\u001b[A\n",
      "Copying files: 455 files [00:04, 92.47 files/s]\u001b[A\n",
      "Copying files: 469 files [00:04, 104.18 files/s]\u001b[A\n",
      "Copying files: 480 files [00:04, 82.99 files/s] \u001b[A\n",
      "Copying files: 490 files [00:05, 81.66 files/s]\u001b[A\n",
      "Copying files: 499 files [00:05, 82.74 files/s]\u001b[A\n",
      "Copying files: 508 files [00:05, 82.34 files/s]\u001b[A\n",
      "Copying files: 517 files [00:05, 81.84 files/s]\u001b[A\n",
      "Copying files: 526 files [00:05, 82.32 files/s]\u001b[A\n",
      "Copying files: 535 files [00:05, 83.33 files/s]\u001b[A\n",
      "Copying files: 544 files [00:05, 79.94 files/s]\u001b[A\n",
      "Copying files: 553 files [00:05, 80.13 files/s]\u001b[A\n",
      "Copying files: 562 files [00:05, 82.43 files/s]\u001b[A\n",
      "Copying files: 572 files [00:05, 86.29 files/s]\u001b[A\n",
      "Copying files: 583 files [00:06, 89.04 files/s]\u001b[A\n",
      "Copying files: 595 files [00:06, 96.94 files/s]\u001b[A\n",
      "Copying files: 606 files [00:06, 100.56 files/s]\u001b[A\n",
      "Copying files: 617 files [00:06, 101.77 files/s]\u001b[A\n",
      "Copying files: 628 files [00:06, 103.63 files/s]\u001b[A\n",
      "Copying files: 640 files [00:06, 107.22 files/s]\u001b[A\n",
      "Copying files: 651 files [00:06, 97.36 files/s] \u001b[A\n",
      "Copying files: 662 files [00:06, 99.81 files/s]\u001b[A\n",
      "Copying files: 673 files [00:06, 92.22 files/s]\u001b[A\n",
      "Copying files: 683 files [00:07, 93.84 files/s]\u001b[A\n",
      "Copying files: 693 files [00:07, 94.26 files/s]\u001b[A\n",
      "Copying files: 703 files [00:07, 91.11 files/s]\u001b[A\n",
      "Copying files: 713 files [00:07, 89.22 files/s]\u001b[A\n",
      "Copying files: 722 files [00:07, 82.56 files/s]\u001b[A\n",
      "Copying files: 731 files [00:07, 62.23 files/s]\u001b[A\n",
      "Copying files: 738 files [00:07, 59.21 files/s]\u001b[A\n",
      "Copying files: 745 files [00:08, 59.40 files/s]\u001b[A\n",
      "Copying files: 752 files [00:08, 60.16 files/s]\u001b[A\n",
      "Copying files: 761 files [00:08, 65.95 files/s]\u001b[A\n",
      "Copying files: 768 files [00:08, 63.77 files/s]\u001b[A\n",
      "Copying files: 775 files [00:08, 61.47 files/s]\u001b[A\n",
      "Copying files: 782 files [00:08, 56.07 files/s]\u001b[A\n",
      "Copying files: 788 files [00:08, 54.54 files/s]\u001b[A\n",
      "Copying files: 794 files [00:08, 54.31 files/s]\u001b[A\n",
      "Copying files: 800 files [00:09, 54.01 files/s]\u001b[A\n",
      "Copying files: 808 files [00:09, 58.68 files/s]\u001b[A\n",
      "Copying files: 818 files [00:09, 68.09 files/s]\u001b[A\n",
      "Copying files: 828 files [00:09, 75.56 files/s]\u001b[A\n",
      "Copying files: 836 files [00:09, 74.79 files/s]\u001b[A\n",
      "Copying files: 844 files [00:09, 72.67 files/s]\u001b[A\n",
      "Copying files: 853 files [00:09, 74.81 files/s]\u001b[A\n",
      "Copying files: 861 files [00:09, 76.09 files/s]\u001b[A\n",
      "Copying files: 870 files [00:09, 79.61 files/s]\u001b[A\n",
      "Copying files: 880 files [00:10, 83.36 files/s]\u001b[A\n",
      "Copying files: 889 files [00:10, 84.12 files/s]\u001b[A\n",
      "Copying files: 898 files [00:10, 82.79 files/s]\u001b[A\n",
      "Copying files: 907 files [00:10, 81.45 files/s]\u001b[A\n",
      "Copying files: 916 files [00:10, 78.02 files/s]\u001b[A\n",
      "Copying files: 928 files [00:10, 87.45 files/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "\n",
    "data_dir = './DS6'\n",
    "splitfolders.ratio(data_dir, output=\"splitted_Data\", seed=1337, ratio=(.8, 0.1,0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbdd87",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Creat Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac13fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Images of Myocardial Infarction Patients', 'Images of Patient that have History of MI', 'Images of Patient that have abnormal heartbeat', 'al Person ECG Images']\n",
      "Number of avaialable classes:  4\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "#batch size\n",
    "batch_size = 20\n",
    "\n",
    "#convert the data to a normalized tensor\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "    transforms.RandomVerticalFlip(),                                       \n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5) , (0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5) , (0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "data_dir = './splitted_Data'\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "valid_dir = os.path.join(data_dir, 'val/')\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=train_transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "#prepare a dict for all loaders\n",
    "loaders ={}\n",
    "loaders['train'] = train_loader\n",
    "loaders['test'] = test_loader\n",
    "loaders['valid'] = valid_loader\n",
    "\n",
    "# get available classes, to be used as the output\n",
    "classes = []\n",
    "for cl in train_data.classes:\n",
    "    classes.append(cl[4:])\n",
    "print(classes)\n",
    "\n",
    "print(\"Number of avaialable classes: \" , len(classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fa669",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Create Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25c6c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    \n",
    "    #load previous model if it exists, to save time\n",
    "    if os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "        \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            #clear optimizer\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            #compute predicted op\n",
    "            output = model(data)\n",
    "        \n",
    "            #calculate error/loss for this batch\n",
    "            loss = criterion(output, target)   \n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            ## record the average training loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            #train_loss += loss.item() * data.size(0) #multiply by batch size\n",
    "            \n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            #in validation we don't do backward propagation\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            #valid_loss += loss.item()* data.size(0)\n",
    "\n",
    "        #train_loss = train_loss / len(loaders['train'].dataset)\n",
    "        #valid_loss = valid_loss / len(loaders['valid'].dataset)\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(\"validation loss decreased {:.6f} --> {:.6f} . saving model .. \".format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict() , save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59092ac7",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Creat Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250ad979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb21d9f",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5: Creat VGG Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf06cb5",
   "metadata": {},
   "source": [
    "#### Creating the pre-trained model, VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75f40c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "output after replacing last layer: \n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Load the pretrained model from pytorch\n",
    "model_VGG = models.vgg16(pretrained=True)\n",
    "\n",
    "# print out the model structure\n",
    "print(model_VGG)\n",
    "\n",
    "for param in model_VGG.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "    \n",
    "n_inputs = model_VGG.classifier[6].in_features\n",
    "\n",
    "# add last linear layer (n_inputs ->ECG classes)\n",
    "last_layer = nn.Linear(n_inputs, len(classes))\n",
    "\n",
    "model_VGG.classifier[6] = last_layer\n",
    "\n",
    "# check to see that your last layer produces the expected number of outputs\n",
    "print(\"output after replacing last layer: \")\n",
    "print(model_VGG.classifier[6].out_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf67e5e",
   "metadata": {},
   "source": [
    "<a id='step6'></a>\n",
    "## Step 6: Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cd55566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "n_epochs =50\n",
    "learning_rate = 0.01\n",
    "criterion_VGG = nn.CrossEntropyLoss()\n",
    "optimizer_VGG = optim.SGD(model_VGG.classifier.parameters(), lr=learning_rate )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ded82",
   "metadata": {},
   "source": [
    "<a id='step7'></a>\n",
    "## Step 7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "341b3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.885222 \tValidation Loss: 0.756611\n",
      "validation loss decreased inf --> 0.756611 . saving model .. \n",
      "Epoch: 2 \tTraining Loss: 0.898096 \tValidation Loss: 1.203647\n",
      "Epoch: 3 \tTraining Loss: 0.930196 \tValidation Loss: 1.726595\n",
      "Epoch: 4 \tTraining Loss: 0.927383 \tValidation Loss: 0.832635\n",
      "Epoch: 5 \tTraining Loss: 0.852272 \tValidation Loss: 0.717500\n",
      "validation loss decreased 0.756611 --> 0.717500 . saving model .. \n",
      "Epoch: 6 \tTraining Loss: 0.836031 \tValidation Loss: 0.821921\n",
      "Epoch: 7 \tTraining Loss: 0.803938 \tValidation Loss: 0.704318\n",
      "validation loss decreased 0.717500 --> 0.704318 . saving model .. \n",
      "Epoch: 8 \tTraining Loss: 0.791025 \tValidation Loss: 0.967690\n",
      "Epoch: 9 \tTraining Loss: 0.858149 \tValidation Loss: 1.160848\n",
      "Epoch: 10 \tTraining Loss: 0.857056 \tValidation Loss: 1.996272\n",
      "Epoch: 11 \tTraining Loss: 0.820636 \tValidation Loss: 0.626259\n",
      "validation loss decreased 0.704318 --> 0.626259 . saving model .. \n",
      "Epoch: 12 \tTraining Loss: 0.824156 \tValidation Loss: 1.975935\n",
      "Epoch: 13 \tTraining Loss: 0.851858 \tValidation Loss: 1.904213\n",
      "Epoch: 14 \tTraining Loss: 0.844023 \tValidation Loss: 0.804341\n",
      "Epoch: 15 \tTraining Loss: 0.905247 \tValidation Loss: 1.936837\n",
      "Epoch: 16 \tTraining Loss: 0.861070 \tValidation Loss: 0.819228\n",
      "Epoch: 17 \tTraining Loss: 0.874194 \tValidation Loss: 1.430759\n",
      "Epoch: 18 \tTraining Loss: 0.887295 \tValidation Loss: 1.020362\n",
      "Epoch: 19 \tTraining Loss: 0.844673 \tValidation Loss: 1.002085\n",
      "Epoch: 20 \tTraining Loss: 0.853605 \tValidation Loss: 0.967635\n",
      "Epoch: 21 \tTraining Loss: 0.764920 \tValidation Loss: 0.735949\n",
      "Epoch: 22 \tTraining Loss: 0.792156 \tValidation Loss: 1.879490\n",
      "Epoch: 23 \tTraining Loss: 0.863615 \tValidation Loss: 1.853734\n",
      "Epoch: 24 \tTraining Loss: 0.847538 \tValidation Loss: 0.737227\n",
      "Epoch: 25 \tTraining Loss: 0.794849 \tValidation Loss: 0.726624\n",
      "Epoch: 26 \tTraining Loss: 0.762091 \tValidation Loss: 0.670494\n",
      "Epoch: 27 \tTraining Loss: 0.816782 \tValidation Loss: 3.859608\n",
      "Epoch: 28 \tTraining Loss: 0.905924 \tValidation Loss: 1.951557\n",
      "Epoch: 29 \tTraining Loss: 0.873241 \tValidation Loss: 0.770243\n",
      "Epoch: 30 \tTraining Loss: 0.795523 \tValidation Loss: 1.696391\n",
      "Epoch: 31 \tTraining Loss: 0.818188 \tValidation Loss: 0.612597\n",
      "validation loss decreased 0.626259 --> 0.612597 . saving model .. \n",
      "Epoch: 32 \tTraining Loss: 0.766331 \tValidation Loss: 0.630670\n",
      "Epoch: 33 \tTraining Loss: 0.824692 \tValidation Loss: 0.782967\n",
      "Epoch: 34 \tTraining Loss: 0.745929 \tValidation Loss: 0.663884\n",
      "Epoch: 35 \tTraining Loss: 0.725427 \tValidation Loss: 3.245309\n",
      "Epoch: 36 \tTraining Loss: 0.812088 \tValidation Loss: 0.733028\n",
      "Epoch: 37 \tTraining Loss: 0.733246 \tValidation Loss: 0.983534\n",
      "Epoch: 38 \tTraining Loss: 0.748738 \tValidation Loss: 0.747577\n",
      "Epoch: 39 \tTraining Loss: 0.816773 \tValidation Loss: 1.237108\n",
      "Epoch: 40 \tTraining Loss: 0.750900 \tValidation Loss: 0.671187\n",
      "Epoch: 41 \tTraining Loss: 0.675681 \tValidation Loss: 0.621864\n",
      "Epoch: 42 \tTraining Loss: 0.743766 \tValidation Loss: 2.526948\n",
      "Epoch: 43 \tTraining Loss: 0.832294 \tValidation Loss: 1.226791\n",
      "Epoch: 44 \tTraining Loss: 0.770300 \tValidation Loss: 0.925055\n",
      "Epoch: 45 \tTraining Loss: 0.776792 \tValidation Loss: 0.863055\n",
      "Epoch: 46 \tTraining Loss: 0.760021 \tValidation Loss: 0.962125\n",
      "Epoch: 47 \tTraining Loss: 0.746789 \tValidation Loss: 0.790841\n",
      "Epoch: 48 \tTraining Loss: 0.693263 \tValidation Loss: 0.779211\n",
      "Epoch: 49 \tTraining Loss: 0.772163 \tValidation Loss: 0.813176\n",
      "Epoch: 50 \tTraining Loss: 0.715453 \tValidation Loss: 0.783858\n"
     ]
    }
   ],
   "source": [
    "model_VGG = train(n_epochs, loaders, model_VGG, optimizer_VGG, criterion_VGG, use_cuda, 'model_VGG.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048b67e",
   "metadata": {},
   "source": [
    "<a id='step8'></a>\n",
    "## Step 8: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21bcf16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy \n",
    "model_VGG.load_state_dict(torch.load('model_VGG.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7d24930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.758010\n",
      "\n",
      "\n",
      "Test Accuracy: 68% (66/96)\n"
     ]
    }
   ],
   "source": [
    "test(loaders, model_VGG, criterion_VGG, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01edcd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
